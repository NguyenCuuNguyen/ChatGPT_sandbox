{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNnqAgkyrCxqwcRBIwDsLry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NguyenCuuNguyen/ChatGPT_sandbox/blob/main/Bert_Not_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dir = '/content/drive/MyDrive/ML_output_scrambled.csv'\n",
        "\n",
        "#https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m98fYNIM2ynf",
        "outputId": "10d4cdca-4fcd-42c5-cdd0-58760856ee21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvpR6T2QqDev",
        "outputId": "1d2a1bbf-ea64-4328-ba3d-31e6bb6c5ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Transformers\n",
        "!pip install transformers\n",
        "# To get model summary\n",
        "!pip install torchinfo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "# specify GPU\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "eGL3ocA32JcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "df = pd.read_csv(dir)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "cDXAGCuc4eq3",
        "outputId": "ecf4d268-7a08-48d4-b7cb-22cc257913f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             CONTENT lang   latitude  \\\n",
              "0  私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...   ja  35.689456   \n",
              "1  私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...   ja  35.089432   \n",
              "2  私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...   ja  35.180226   \n",
              "3  私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...   ja  39.852086   \n",
              "4  私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...   ja  35.607474   \n",
              "\n",
              "    longitude                      created_at COUNTRY               id_str  \\\n",
              "0  139.691716  Thu Jul 21 00:19:55 +0000 2022   Japan  1549912016466640896   \n",
              "1  137.074866  Thu Jul 21 00:20:34 +0000 2022   Japan  1549912178635214849   \n",
              "2  136.906739  Thu Jul 21 00:18:24 +0000 2022   Japan  1549911636978573313   \n",
              "3  141.164859  Thu Jul 21 00:18:12 +0000 2022   Japan  1549911583991955456   \n",
              "4  140.106628  Thu Jul 21 00:17:47 +0000 2022   Japan  1549911478597459968   \n",
              "\n",
              "                                     cleaned_content    target color  \n",
              "0  子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...  Anti-Vax   red  \n",
              "1  子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...  Anti-Vax   red  \n",
              "2  子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...  Anti-Vax   red  \n",
              "3  子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...  Anti-Vax   red  \n",
              "4  子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...  Anti-Vax   red  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-ca868f78-64dd-4f8f-9ca4-2f761fd26910\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>lang</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>created_at</th>\n",
              "      <th>COUNTRY</th>\n",
              "      <th>id_str</th>\n",
              "      <th>cleaned_content</th>\n",
              "      <th>target</th>\n",
              "      <th>color</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...</td>\n",
              "      <td>ja</td>\n",
              "      <td>35.689456</td>\n",
              "      <td>139.691716</td>\n",
              "      <td>Thu Jul 21 00:19:55 +0000 2022</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1549912016466640896</td>\n",
              "      <td>子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...</td>\n",
              "      <td>Anti-Vax</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...</td>\n",
              "      <td>ja</td>\n",
              "      <td>35.089432</td>\n",
              "      <td>137.074866</td>\n",
              "      <td>Thu Jul 21 00:20:34 +0000 2022</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1549912178635214849</td>\n",
              "      <td>子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...</td>\n",
              "      <td>Anti-Vax</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...</td>\n",
              "      <td>ja</td>\n",
              "      <td>35.180226</td>\n",
              "      <td>136.906739</td>\n",
              "      <td>Thu Jul 21 00:18:24 +0000 2022</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1549911636978573313</td>\n",
              "      <td>子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...</td>\n",
              "      <td>Anti-Vax</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...</td>\n",
              "      <td>ja</td>\n",
              "      <td>39.852086</td>\n",
              "      <td>141.164859</td>\n",
              "      <td>Thu Jul 21 00:18:12 +0000 2022</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1549911583991955456</td>\n",
              "      <td>子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...</td>\n",
              "      <td>Anti-Vax</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>私達が子供のときは新型コロナウイルスなんて存在しなかったし、\\nワクチンがなかったので、ヒブ...</td>\n",
              "      <td>ja</td>\n",
              "      <td>35.607474</td>\n",
              "      <td>140.106628</td>\n",
              "      <td>Thu Jul 21 00:17:47 +0000 2022</td>\n",
              "      <td>Japan</td>\n",
              "      <td>1549911478597459968</td>\n",
              "      <td>子供 新型 コロナ ウイルス 存在 ワクチン 肺炎 球菌 髄膜 子供 命 落とす 麻疹 ワク...</td>\n",
              "      <td>Anti-Vax</td>\n",
              "      <td>red</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca868f78-64dd-4f8f-9ca4-2f761fd26910')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-56985a06-608c-4a78-8b87-d97cebac2c2b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56985a06-608c-4a78-8b87-d97cebac2c2b')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-56985a06-608c-4a78-8b87-d97cebac2c2b button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca868f78-64dd-4f8f-9ca4-2f761fd26910 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca868f78-64dd-4f8f-9ca4-2f761fd26910');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts()\n",
        "# Converting the labels into encodings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['target'] = le.fit_transform(df['target'])\n",
        "# check class distribution\n",
        "df['target'].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl6GlBr85Kyy",
        "outputId": "ceb2d443-6152-4a69-c943-8d40a2216c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.582669\n",
              "2    0.382539\n",
              "1    0.034791\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = df['CONTENT'], df['target']"
      ],
      "metadata": {
        "id": "xqI43S4Y7QgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModel, BertTokenizerFast\n",
        "# # Load the BERT tokenizer\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# # Import BERT-base pretrained model\n",
        "# bert = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "tOCsvl5d6raf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# # Load the Roberta tokenizer\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# # Import Roberta pretrained model\n",
        "# bert = RobertaModel.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "1EYCs9MP63zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "# Load the DistilBert tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('karolill/distilmbert_LR3e-05_WR0.1_OPTIMadamw_hf_WD0.01')#('distilbert-base-uncased')\n",
        "# Import the DistilBert pretrained model\n",
        "bert = DistilBertModel.from_pretrained('karolill/distilmbert_LR3e-05_WR0.1_OPTIMadamw_hf_WD0.01')#('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "v53MSfoK65Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max_seq_len is the longest lenght of 1 data point\n",
        "tokens_train = tokenizer(\n",
        "    train_text.tolist(),\n",
        "    max_length = 512,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_6Iv9F27oNU",
        "outputId": "1cb9c442-3e37-4b4a-997c-865aa8fc0fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert int sequence to tensor\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())"
      ],
      "metadata": {
        "id": "GEbpQSr48CXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create dataloaders for the training set. These dataloaders will pass batches of train data as input to the model during the training phase."
      ],
      "metadata": {
        "id": "GhyNrXOp8KgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#define a batch size\n",
        "batch_size = 16\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rwXGZEeM8EZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "   def __init__(self, bert):\n",
        "       super(BERT_Arch, self).__init__()\n",
        "       self.bert = bert\n",
        "\n",
        "       # dropout layer\n",
        "       self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "       # relu activation function\n",
        "       self.relu =  nn.ReLU()\n",
        "       # dense layer\n",
        "       self.fc1 = nn.Linear(768,512)\n",
        "       self.fc2 = nn.Linear(512,256)\n",
        "       self.fc3 = nn.Linear(256,3)\n",
        "       #softmax activation function\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "       #define the forward pass\n",
        "   def forward(self, sent_id, mask):\n",
        "      #pass the inputs to the model\n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
        "\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      # output layer\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "VOFQ-EFN8OMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n",
        "for param in bert.parameters():\n",
        "      param.requires_grad = False\n",
        "model = BERT_Arch(bert)\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D6VQf1T8Q5f",
        "outputId": "97d56e26-7980-41fe-e285-97ad4dbf519f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "BERT_Arch                                               --\n",
              "├─DistilBertModel: 1-1                                  --\n",
              "│    └─Embeddings: 2-1                                  --\n",
              "│    │    └─Embedding: 3-1                              (91,812,096)\n",
              "│    │    └─Embedding: 3-2                              (393,216)\n",
              "│    │    └─LayerNorm: 3-3                              (1,536)\n",
              "│    │    └─Dropout: 3-4                                --\n",
              "│    └─Transformer: 2-2                                 --\n",
              "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
              "├─Dropout: 1-2                                          --\n",
              "├─ReLU: 1-3                                             --\n",
              "├─Linear: 1-4                                           393,728\n",
              "├─Linear: 1-5                                           131,328\n",
              "├─Linear: 1-6                                           771\n",
              "├─LogSoftmax: 1-7                                       --\n",
              "================================================================================\n",
              "Total params: 135,259,907\n",
              "Trainable params: 525,827\n",
              "Non-trainable params: 134,734,080\n",
              "================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the Optimizer we reduce the loss during backpropagation through the network.\n",
        "from transformers import AdamW\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xQmBkfL8TuH",
        "outputId": "c51f5975-df66-402b-8e15-edb8d31b8554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balancing the weights while calculating the error"
      ],
      "metadata": {
        "id": "iC2QwVkE8noR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H9zsbueS9RBk",
        "outputId": "0ce2731d-1b36-419b-b9fd-2ea86506f283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels\n",
        "                                    )\n",
        "print(class_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukT2riU38lL3",
        "outputId": "05ff4ed8-83fc-43f0-a891-d105493e2a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.57207961 9.58099688 0.87136988]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Balancing the weights while calculating the error\n",
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "# loss function\n",
        "cross_entropy = nn.NLLLoss(weight=weights)"
      ],
      "metadata": {
        "id": "Sap-Wtnk9-Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the epochs\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "train_losses=[]\n",
        "# number of training epochs\n",
        "epochs = 200\n",
        "# We can also use learning rate scheduler to achieve better results\n",
        "lr_sch = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
      ],
      "metadata": {
        "id": "4LR1jipZ-Dcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune model"
      ],
      "metadata": {
        "id": "sc1LQbnk-c8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # clear calculated gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # We are not using learning rate scheduler as of now\n",
        "    # lr_sch.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "rTvtAnkz-cMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJtBGihn-v0A",
        "outputId": "c0bc4406-e856-40f6-8464-5a677cbc36b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 2 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 3 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 4 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 5 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 6 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 7 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 8 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 9 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 10 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 11 / 200\n",
            "\n",
            "Training Loss: 0.006\n",
            "\n",
            " Epoch 12 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 13 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 14 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 15 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 16 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 17 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 18 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 19 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 20 / 200\n",
            "\n",
            "Training Loss: 0.007\n",
            "\n",
            " Epoch 21 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 22 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 23 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 24 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 25 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 26 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 27 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 28 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 29 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 30 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 31 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 32 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 33 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 34 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 35 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 36 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 37 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 38 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 39 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 40 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 41 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 42 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 43 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 44 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 45 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 46 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 47 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 48 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 49 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 50 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 51 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 52 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 53 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 54 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 55 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 56 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 57 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 58 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 59 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 60 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 61 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 62 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 63 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 64 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 65 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 66 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 67 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 68 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 69 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 70 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 71 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 72 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 73 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 74 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 75 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 76 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 77 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 78 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 79 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 80 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 81 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 82 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 83 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 84 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 85 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 86 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 87 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 88 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 89 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 90 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 91 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 92 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 93 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 94 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 95 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 96 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 97 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 98 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 99 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 100 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 101 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 102 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 103 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 104 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 105 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 106 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 107 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 108 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 109 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 110 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 111 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 112 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 113 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 114 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 115 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 116 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 117 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 118 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 119 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 120 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 121 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 122 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 123 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 124 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 125 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 126 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 127 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 128 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 129 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 130 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 131 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 132 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 133 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 134 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 135 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 136 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 137 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 138 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 139 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 140 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 141 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 142 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 143 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 144 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 145 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 146 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 147 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 148 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 149 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 150 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 151 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 152 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 153 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 154 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 155 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 156 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 157 / 200\n",
            "\n",
            "Training Loss: 0.005\n",
            "\n",
            " Epoch 158 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 159 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 160 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 161 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 162 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 163 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 164 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 165 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 166 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 167 / 200\n",
            "\n",
            "Training Loss: 0.006\n",
            "\n",
            " Epoch 168 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 169 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 170 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 171 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 172 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 173 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 174 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 175 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 176 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 177 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 178 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 179 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 180 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 181 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 182 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 183 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 184 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 185 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 186 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 187 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 188 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 189 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 190 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 191 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 192 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 193 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 194 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 195 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 196 / 200\n",
            "\n",
            "Training Loss: 0.003\n",
            "\n",
            " Epoch 197 / 200\n",
            "\n",
            "Training Loss: 0.001\n",
            "\n",
            " Epoch 198 / 200\n",
            "\n",
            "Training Loss: 0.002\n",
            "\n",
            " Epoch 199 / 200\n",
            "\n",
            "Training Loss: 0.004\n",
            "\n",
            " Epoch 200 / 200\n",
            "\n",
            "Training Loss: 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cpu()\n",
        "torch.save(model, \"bert_model.pt\")\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "Xn3UAClyE1jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(str):\n",
        "  str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
        "  test_text = [str]\n",
        "  model.eval()\n",
        "\n",
        "  tokens_test_data = tokenizer(\n",
        "  test_text,\n",
        "  max_length = 512,\n",
        "  pad_to_max_length=True,\n",
        "  truncation=True,\n",
        "  return_token_type_ids=False\n",
        "  )\n",
        "  test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
        "\n",
        "  preds = None\n",
        "  with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  print('Intent Identified: ', le.inverse_transform(preds)[0])\n",
        "  return le.inverse_transform(preds)[0]\n",
        "\n",
        "\n",
        "def get_response(message):\n",
        "  intent = get_prediction(message)\n",
        "  for i in df['target']:\n",
        "    if i[\"tag\"] == intent:\n",
        "      result = random.choice(i[\"target\"])\n",
        "      break\n",
        "  print(f\"Response : {result}\")\n",
        "  return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"
      ],
      "metadata": {
        "id": "lSM7Q0cV-wPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = torch.load(\"bert_model.pt\")"
      ],
      "metadata": {
        "id": "bD8QdmflFKHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEED TO ADD TAG TO THE COLUMNS\n",
        "df[\"target\"] = [[0,1,2]]*len(df)\n",
        "get_prediction(\"ワクチンが好きです\")\n",
        "get_prediction(\"I like vaccine\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Fxa-35bOCyRt",
        "outputId": "1d39e50a-572c-483f-abbe-9f3efcca298d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Identified:  Pro-Vax\n",
            "Intent Identified:  Anti-Vax\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Anti-Vax'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "oEEZ0mtHAjJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add test data to get new predictions"
      ],
      "metadata": {
        "id": "Q7an48gyEbwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/Tweepy_data_clean.csv')\n",
        "test_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ld3d8dzTBZ64",
        "outputId": "247a539b-4f88-4f75-ce32-25c3b9e04bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content  \\\n",
              "0  .@MLongMD with great comprehensive #immunizati...   \n",
              "1  PSA: Let’s get rid of the stigma surrounding #...   \n",
              "2  @MartianPontiac @NoyesJHumphrey @JanPeterBalke...   \n",
              "3  SJM has strong reasons to believe that HPV may...   \n",
              "4  Henrietta Lacks cells(HeLa cells) were taken u...   \n",
              "\n",
              "                       created_at             user_location        id_str  \n",
              "0  Tue Feb 22 21:43:52 +0000 2022          Jacksonville, FL  1.496240e+18  \n",
              "1  Tue Feb 22 18:58:10 +0000 2022  Perth, Western Australia  1.496200e+18  \n",
              "2  Tue Feb 22 19:12:35 +0000 2022                  New York  1.496200e+18  \n",
              "3  Tue Feb 22 17:54:21 +0000 2022              Delhi, India  1.496180e+18  \n",
              "4  Tue Feb 22 18:56:17 +0000 2022                    Alaska  1.496200e+18  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-81f73f4f-8018-4e66-b52a-98a407f56c51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>created_at</th>\n",
              "      <th>user_location</th>\n",
              "      <th>id_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@MLongMD with great comprehensive #immunizati...</td>\n",
              "      <td>Tue Feb 22 21:43:52 +0000 2022</td>\n",
              "      <td>Jacksonville, FL</td>\n",
              "      <td>1.496240e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PSA: Let’s get rid of the stigma surrounding #...</td>\n",
              "      <td>Tue Feb 22 18:58:10 +0000 2022</td>\n",
              "      <td>Perth, Western Australia</td>\n",
              "      <td>1.496200e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MartianPontiac @NoyesJHumphrey @JanPeterBalke...</td>\n",
              "      <td>Tue Feb 22 19:12:35 +0000 2022</td>\n",
              "      <td>New York</td>\n",
              "      <td>1.496200e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SJM has strong reasons to believe that HPV may...</td>\n",
              "      <td>Tue Feb 22 17:54:21 +0000 2022</td>\n",
              "      <td>Delhi, India</td>\n",
              "      <td>1.496180e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Henrietta Lacks cells(HeLa cells) were taken u...</td>\n",
              "      <td>Tue Feb 22 18:56:17 +0000 2022</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1.496200e+18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81f73f4f-8018-4e66-b52a-98a407f56c51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-6a616ae7-d85e-408c-a1c5-77fe84d1c0f3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a616ae7-d85e-408c-a1c5-77fe84d1c0f3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-6a616ae7-d85e-408c-a1c5-77fe84d1c0f3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81f73f4f-8018-4e66-b52a-98a407f56c51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81f73f4f-8018-4e66-b52a-98a407f56c51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuxW_VvNKwxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}